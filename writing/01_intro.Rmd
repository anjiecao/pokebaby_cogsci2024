
From birth, humans learn actively. Even before they can move on their own, infants can select information by deciding what to look at and when to stop looking [@haith1980rules; @raz2020learning]. Developmental psychologists have long leveraged this attentional decision-making to make inferences about the perceptual and cognitive abilities of infants by measuring how long infants look at certain stimuli [@aslin2007s; @baillargeon1985object; @fantz1963pattern]. 

Two key phenomena are particularly critical for these inferences: habituation and dishabituation. Habituation refers to the decrease in looking time upon seeing the same or similar stimuli repeatedly; dishabituation refers to the increase in looking time following presentation of a novel stimulus after habituation. In order to dishabituate, the infant must distinguish between the original stimulus and the novel one.  While habituation and dishabituation have been robustly documented, the underlying mechanisms of these looking time changes remain poorly understood. In this paper, we address this gap by presenting a rational model that provides principled predictions on the magnitude of dishabituation. Critically, this model can be applied generally to make predictions about looking time for arbitrary stimuli by using  embeddings derived from a convolutional neural network.

The dominant framework in explaining changes in looking time proposes that habituation and dishabituation are driven by the amount of information to be encoded in the stimulus [@hunter1988multifactor]. Observers look longer at a stimulus if the stimulus has a lot of unencoded information, and as exposure to the stimulus accumulates, less information is left unencoded, leading to shorter looking time. While this theory has been highly influential, the lack of formal details about what is meant by “encoding” opens the door for post-hoc interpretation of looking time measurements. A stimulus could be argued to be novel because it has distinct perceptual features, but it could also be familiar because of its conceptual characteristics. In part as a result of this interpretive ambiguity, concerns have been raised about  looking time measurements should be the foundation for central claims in developmental psychology [@blumberg2023protracted; @haith1998put; @paulus2022should]. 

Computational models provide an important tool for formalizing the details of the habituation and dishabituation process. One set of models describe infants’ looking behaviors with information-theoretic measures derived from ideal observer models [@kidd2012goldilocks; @poli2020infants; @poli2023eight]. For example, @poli2023eight developed a model that calculates the Kullback–Leibler (KL) divergence of each event infants saw in an experiment.  This measure was shown to predict infants’ looking time, with higher KL links to longer looking time. 

While these models provided quantitative accounts of the habituation process, they do not model the infant’s information sampling process directly. Instead, they describe trial-level correlations between these information-theoretic measures and measured looking times. In other words, these models do not explain how information theoretics measures are causally related to the sampling decision at any given moment. Furthermore, these prior models presuppose an abstracted representation of the stimuli, and do not instantiate a precise hypothesis about how visual encoding occurs during attentional decision-making. This limits the ability of these models to make principled predictions on new, unseen stimuli.

To address these issues, Rational Action, Noisy Choice for Habituation (RANCH) was developed [@cao2023habituation; @raz2023modeling].  RANCH described an agent’s looking behavior as a rational exploration of noisy perceptual samples. This model construes the looking time paradigm as a series of binary decisions: to keep sampling from the current stimulus, or to move on to the next stimulus. This model makes sampling decisions based on the Expected Information Gain (EIG) of the perceptual samples, and therefore can be seen as a rational analysis of looking behavior [@anderson1991human; @lieder2020resource; @oaksford1994rational]. 

Newer versions of RANCH also incorporate recent progress in convolutional neural networks, which have offered insights into how the visual system encodes objects [@doshi2023cortical; @hebart2020revealing; @yamins2014performance]. The activations of these brain-inspired neural networks form embedding spaces, each of which can be seen as a quantitative hypothesis about the representations that humans form for visual stimuli [@schrimpf2020integrative]. For example, @lee2022rapid projected the final layer of a ResNet50 into a "perceptually-aligned" space, by making its representations match dissimilarity matrices derived from human adult reaction times in a 2-AFC match-to-sample task. Passing new stimuli through this perceptual alignment yields a possible representation of how humans embed different visual stimuli in a low-dimensional space. Using this perceptually-aligned embedding space as a model of perceptual encoding, RANCH can generate fine-grained predictions about how long observers will look at stimuli sequence previously unseen by the model in the training.

Previously, @cao2023habituation and @raz2023modeling have shown that RANCH can successfully model habituation and dishabituation in adults and infants.  Here, we test RANCH’s ability to (1) predict responses in new data, and (2) predict a key phenomenon in qualitative accounts of habituation.  For the first test,  we tune its parameters to behavior on a generic habituation and dishabituation task, and test its performance on a previously unseen behavioral dataset. For the second test, we focus on RANCH’s ability to reproduce a prediction from @hunter1988multifactor model of habituation and dishabituation: that observers’ dishabituation magnitude should be related to the similarity between the habituated stimulus and the novel stimulus. The more dissimilar two stimuli are, the more one should dishabituate to the novel stimulus. 


To conduct these tests, we first fit RANCH’s parameters to a habituation-dishabituation experiment in which participants saw sequences of monsters which were either familiar or novel [dataset reported in @cao2023habituation, Fig 1A]. Then, we use the best-fitting parameters to generate predictions for a new experiment that measure the subtle differences in dishabituation magnitude based on stimulus similarity (Fig 1C). In this experiment, we systematically varied the similarity between habituation and dishabituation stimuli such that dishabituation stimuli differed in their pose angle, their number, their identity, or their animacy. 

To preview our results, we show that RANCH can predict looking time responses in new data just by transferring model parameters fit from previous data, with marginal differences in performance compared to completely refitting to the new data. RANCH also captured the particular ordering of the dishabituation magnitude as a function of stimulus dissimilarity, thereby predicting a novel qualitative phenomenon (graded dishabituation) without ever being trained on it. Finally, we show that RANCH is relatively robust across parameter settings, but the assumptions about its perceptual representation, learning process, and the decision process are all critical to its performance.

