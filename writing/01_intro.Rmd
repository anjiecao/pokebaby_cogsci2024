Since birth, humans learn actively by deciding what to look at and when to stop looking [@haith1980rules, @raz2020learning]. Developmental psychologists have long leveraged this attentional decision-making to make inferences about the perceptual and cognitive abilities of infants by measuring how long infants look at certain stimuli [@aslin2007s, @baillargeon1985object, @fantz1963pattern]. There are two key phenomena that are particularly critical for these inferences: habituation and dishabituation. Habituation refers to the decrease in looking time upon seeing repeated stimuli, whereas dishabituation refers to the increase in looking time after habituation. When participants dishabituate to a stimulus, they are thought to have distinguished between the novel stimulus and the stimulus they habituated to. Differences in dishabituation magnitude between different stimuli forms the basis of inferences about different levels of surprise that participants are experiencing.  While habituaiton and dishabituaiton have been robustly documented, the underlying mechanisms of these looking time changes remain poorly understood.  Past work mostly focus on the qualitative features (i.e. Looking time is shorter versus longer) and leaves quantitative details of this phenomenon. In this paper, we address this gap by presenting a rational model that provides principled predictions on the magnitude of dishabituation based on perceptual embedding.

The dominant framework in explaining changes in looking time proposes that habituation and dishabituation happen due to the amount of information to be encoded in the stimulus (Hunter & Ames, 1988). One would look longer at a stimulus if the stimulus has a lot of unencoded information, and as exposure to the stimulus accumulates, less information is left unencoded, leading to shorter looking time. While this theory is widely cited, the lack of formal details about what is meant by “encoding” opens the door for vague, post-hoc interpretations on looking time measurements. A stimulus could be argued as novel because it is showing distinct perceptual features, but it could also be familiar because of its conceptual characteristics. As a result, there have been concerns raised about whether developmental psychology should trust looking time measurements as the foundation for central claims in developmental psychology [@blumberg2023protracted, @haith1998put, @paulus2022should]. 

To move beyond the current state, many have proposed computational models that offer a mechanistic account of looking time changes. Computational models are a useful tool for developmental science: it forces researchers to spell out assumptions often implicit and vague in verbal theories, and provides quantitative and testable hypotheses about specific phenomena [@liu2017ten, @teglas2011pure]. Some models have tried to formalize encoding by linking infants’ looking behaviors with information-theoretic measures derived from ideal-learner models [@kidd2012goldilocks,poli2020infants, poli2023eight]. While these models have provided quantitative accounts of encoding, they do not model the sampling process directly. Instead, they describe trial-level correlations between these information-theoretic measures and looking To address this, a recent model, RANCH (Rational Action, Noisy Choice for Habituation), models looking behavior as a rational exploration of noisy perceptual samples [@cao2023habituation]). This model simplified the looking time paradigm as a series of binary decisions: to keep sampling from the current stimulus, or to move on to the next stimulus. RANCH makes sampling decisions based on the Expected Information Gain (EIG) of the perceptual samples, and therefore can be seen as a rational analysis of looking behavior [@anderson1991human, @lieder2020resource, @oaksford1994rational]. @cao2023habituation showed that RANCH can accurately capture adults’ habituation and dishabituation trajectories in a self-paced looking time task

However, RANCH suffers from one critical limitation: it lacks a principled way of representing visual stimuli. In RANCH, stimuli are represented as a collection of hand-crafted binary features. This oversimplification not only makes it challenging to test more fine-grained predictions on novel stimuli, but also skips a critical question: how are visual stimuli encoded and represented in the mind? 

To represent stimuli in a psychologically plausible way, we turn to recent progress in neuroscience and brain-inspired deep neural networks, which has offered insights into how we encode visual objects [@doshi2023cortical,@hebart2020revealing, @yamins2014performance]. The activations of these brain-inspired neural networks form embedding spaces, each of which can be seen as quantitative hypotheses about the representations that humans form of objects [@schrimpf2020integrative]. For example, @lee2022rapid projected the final layer of a ResNet50 into a "perceptually-aligned" space, by making its representations match dissimilarity matrices derived from human adult reaction times in a 2-AFC match-to-sample task. Passing new stimuli through this perceptual alignment is a possible representation of how humans embed different visual stimuli in a low-dimensional space.

In light of these advancements, we propose to extend the RANCH model to contain a visual perception component: instead of learning from hand-specified binary vectors, RANCH learns from psychologically informed stimulus embeddings. By doing so, the model allows us to generate fine-grained predictions about how long people will look at previously unseen sequences of stimuli. In particular, we are interested whether RANCH will recapitulate patterns of habituation and dishabituation seen in adults. One hypothesis stemmed from \@hunter1988multifactor is that dishabituation magnitude should be related to the similarity between the habituated stimulus and the novel stimulus: the more dissimilar two stimuli are, the more one should dishabituate to the novel stimulus. 

To do so, we ran adults through a habituation-dishabituation experiment in which we vary the way in which the dishabituation stimulus varies from the habituation stimulus, from subtle violations of pose to more extreme violations of object category. We then ask whether dishabituation is graded: whether the extent to which the violation deviates from habituation stimulus determines the magnitude of dishabituation. We then leverage RANCH’s new ability to make fine-grained predictions by embedding our stimuli into a low-dimensional perceptual space, and forming a perceptual concept based on noisy samples from these embeddings. By subjecting RANCH to the same experiment as participants in our task, we can ask whether it too generates a graded dishabituation signature. 


To preview our results, we indeed find that adults dishabituate more to stimuli that are more different from the stimulus they have habituated to. We also show that RANCH can successfully capture the habituation and dishabituation patterns, including the ordering of the dishabituation magnitude with varying degrees of stimulus dissimilarity. Moreover, we found that RANCH’s performance is relatively robust across parameters, and its best-fitting parameter can be generalized to other tasks. 
