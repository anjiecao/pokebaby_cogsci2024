
```{r}
participant_n <- readRDS(here("cached_data/writing_cache/participant_n.Rds"))
lmer_background_baseline <- readRDS(here("cached_data/writing_cache/lmer_background_baseline.Rds")) %>% mutate(across(is.numeric, round, 2))
lmer_animacy_baseline <- readRDS(here("cached_data/writing_cache/lmer_animacy_baseline.Rds")) %>% mutate(across(is.numeric, round, 2))
lmer_pose_baseline <- readRDS(here("cached_data/writing_cache/lmer_pose_baseline.Rds")) %>% mutate(across(is.numeric, round, 2))
lmer_number_baseline <- readRDS(here("cached_data/writing_cache/lmer_number_baseline.Rds")) %>% mutate(across(is.numeric, round, 2))
lmer_identity_baseline <- readRDS(here("cached_data/writing_cache/lmer_identity_baseline.Rds")) %>% mutate(across(is.numeric, round, 2))
behavior_qua_order <- readRDS(here("cached_data/writing_cache/clean_behavioral_qual_order.Rds")) %>% mutate(across(is.numeric, round, 2))
```


Next, we introduce the novel experiment used to test the generalizability of the parameter setting found above. The procedure was similar to the previous training data with two key differences: First, stimuli used in the training data were animated monsters, whereas the current experiment (test data) used images of animals and vegetables with added shaking animations. Second, unlike the training data, the current experiment varied the dissimilarity between the repeating stimuli and the deviant stimuli by introducing four different types of deviant stimuli. 


## Methods

### Stimuli

All stimuli were created using images selected from Unity assets [“Quirky Series - Animals”](https://assetstore.unity.com/packages/3d/characters/animals/quirky-series-animals-mega-pack-vol-2-183280) and [“3D Prop Vegetables and Fruits”](https://assetstore.unity.com/packages/3d/props/food/3d-prop-vegetables-and-fruits-237790). We added the same minor shaking animation to each image to increase interest. For each image from the animal set, we created a mirrored version to create an image with a different pose. The long axis of each vegetable image was tilted before mirroring. 


### Procedure 

The experiment was a self-paced web-based looking time experiment (Fig. 1C). Participants saw 24 blocks that consisted of either two, four, or six trials. On each trial, a schematic screen would rise up to reveal a stimulus behind it. Participants pressed the spacebar to go on to the next trial after a minimum viewing time of 500 ms, triggering the schematic screen to drop and raise again to reveal the next stimulus. 

Each participant saw eight types of repeating stimuli. The eight types of stimuli included all combinations of the three features that each included two levels: animacy (e.g. animate or inanimate), number (singleton or pair), and pose (facing left or right). 

Eight blocks consisted of one stimulus being repeatedly presented throughout the block (background blocks). The remaining sixteen blocks included two stimuli, including one that was repeatedly presented and one that deviated from the repeating trial. The deviant trial was always different from the repeating stimulus in one of the four dimensions: animacy, identity, number, and pose. The deviant trial always appeared in the last trial of the block. In the first three violation types, the feature would be switched to the previously unseen level (e.g. an inanimate deviant after an animate repeating stimulus, keeping the number and the pose the same). For identity violations, the participants would see a different, but within-category, exemplar from the repeating stimulus category. Among the sixteen blocks, the four violation types each appeared four times. After each block, participants performed a filler task in which they judged whether they had seen an animation before. Half of the filler task showed previously unseen animation, and the other half showed an animation from the preceding block.


To control the distribution of background blocks and deviant blocks, we grouped the 24 blocks into four groups. Each group consisted of two background blocks, and one deviant block from each violation type. The order of blocks within each group was randomized.


### Participants 

We recruited 550 adult participants on Prolific. Participants were excluded if either (1) the standard deviation of log-transformed of their reaction times on all trials was less than 0.15 (indicating key-smashing); (2) they spent more than three absolute deviations above the median of the task completion time as reported by Prolific, or (3) they provided the incorrect response to more than 20% of the memory task.  In total, `r round((550 - participant_n) / 550, 2) * 100` % of the participants were excluded by these criteria. After the participant-level exclusion, we also applied trial-level exclusion. A trial was excluded from final analysis if it was three absolute deviations away from the median in the log-transformed space across all participants. The final sample included `r participant_n` participants.

## Results

The sample size and analysis plan were all pre-registered and can be found [here](https://aspredicted.org/blind.php?x=WGF_J7K). All analysis scripts are publicly available and can be found [here](https://anonymous.4open.science/r/pokebaby_cogsci2024-3636/README.md).

We were primarily interested in (1) whether our experimental paradigm captured habituation and dishabituation and (2) whether the magnitude of dishabituation was influenced by the type of violation.  We tested these two hypotheses in a linear mixed-effect model with maximal random effect structure that predicted log-transformed looking time with the following specification on the fixed effects: `log(total_rt)` $\sim$ `trial_number +  is_first_trial +  (trial_number + is_first_trial) * stimulus_number + (trial_number + is_first_trial) * stimulus_pose +(trial_number + is_first_trial) * stimulus_animacy + (trial_number + is_first_trial) * violation_type + log(block_number)`. The `violation_type`  has five levels, including the background trial and four types of violation. ^[In our preregistered model, we specified an interaction between trial_number and is_first_trial that was automatically removed in the final model.]

To examine the specific contrast between different violations, we set different reference levels for `violation_type`. We found evidence for habituation and graded dishabituation using this technique (Fig. 2). When the background test trial was treated as the reference level, there was a significant effect of trial number, suggesting participants were habituating to the stimuli ($\beta$ = `r filter(lmer_background_baseline, term == "trial_number")$estimate`, *SE* = `r filter(lmer_background_baseline, term == "trial_number")$std.error`, *p* < .001). Moreover, looking time to animacy violations was significantly longer than to number violations ($\beta$ = `r filter(lmer_number_baseline, term == "violation_type_with_backgroundanimacy")$estimate`, *SE* = `r filter(lmer_number_baseline, term == "violation_type_with_backgroundanimacy")$std.error`, *p* < .001) and pose violations ($\beta$ = `r filter(lmer_pose_baseline, term == "violation_type_with_backgroundanimacy")$estimate`, *SE* = `r filter(lmer_pose_baseline, term == "violation_type_with_backgroundanimacy")$std.error`, *p* < .001), and so were identity violations (cf. number: $\beta$ = `r filter(lmer_number_baseline, term == "violation_type_with_backgroundidentity")$estimate`, *SE* = `r filter(lmer_number_baseline, term == "violation_type_with_backgroundidentity")$std.error`, *p* < .001; cf. pose: $\beta$ = `r filter(lmer_pose_baseline, term == "violation_type_with_backgroundidentity")$estimate`, *SE* = `r filter(lmer_pose_baseline, term == "violation_type_with_backgroundidentity")$std.error`, *p* < .001).  But animacy violation was not different from the identity violation, nor was the number violation different from the pose violation (all p > 0.1) 

We also pre-registered a qualitative prediction on the ordering of the dishabituation magnitude (i.e. animacy > number > identity > pose). However, we did not find evidence consistent with this prediction. The qualitative ordering in our data was animacy (*M* = `r behavior_qua_order$animacy`), identity (*M* = `r behavior_qua_order$identity`), pose (*M* = `r behavior_qua_order$pose`) and number (*M* = `r behavior_qua_order$number`). 





