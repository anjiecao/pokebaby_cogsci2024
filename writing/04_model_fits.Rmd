```{r}
## generalizability 
eig_gen_stats <- readRDS(here("cached_data/writing_cache/clean_eig_gen_stats.Rds")) %>% mutate(across(is.numeric, round, 2))
eig_qual_order <- readRDS(here("cached_data/writing_cache/clean_eig_gen_qual_order.Rds"))

# robustness
kfold_eig_params <- readRDS(here("cached_data/eig_aligned_fit_res.Rds"))
best_fit_id <- (kfold_eig_params %>% arrange(-mean_rsquared))$param_id[[1]]
# looking up params manually from other file
all_params <- read_csv(here("data/model_sim_data/sim_info/eig_selfpaced.csv"))
eig_stats <- readRDS(here("cached_data/writing_cache/clean_eig_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
all_eig_params <- readRDS(here("cached_data/eig_nofold_fit.Rds"))
n_params <- nrow(all_eig_params)
eig_rsquared_mean <- round(mean(all_eig_params$rsquared), 2)
eig_rsquared_sd <- round(sd(all_eig_params$rsquared), 2)
eig_rmse_mean <- round(mean(all_eig_params$rmse), 2)
eig_rmse_sd <- round(sd(all_eig_params$rmse, na.rm = TRUE), 2)
eig_qual_order <- readRDS(here("cached_data/writing_cache/clean_eig_qual_order.Rds"))

# alternative models 
nonoise_stats <- readRDS(here("cached_data/writing_cache/clean_nonoise_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
nolearning_stats <- readRDS(here("cached_data/writing_cache/clean_nolearning_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
randemb_stats <- readRDS(here("cached_data/writing_cache/clean_randemb_stats.Rds"))%>% mutate(across(is.numeric, round, 2))

```




We evaluate RANCH in three ways. First, we evaluate whether RANCH can make parameter-free predictions on the new data that it has not been trained on. Second, we investigate to what extent RANCH's performance is robust across parameter settings. Finally, we examined to what extent each component of RANCH is critical to predicting human behaviors. 


## Parameter generalizability 

The procedure to model the current experiment is very similar to the training procedure. We first converted the raw images into the perceptual representations. Then, we assembled the stimuli into the sequences that participants saw in each block. For the blocks with deviating stimuli, we sampled deviant stimuli from the corresponding violation categories. We sampled 23 stimulus pairs for each combination of violation type and deviant position.

However, instead of searching for a new set of best-fitting parameters, we used the best parameters found in the training dataset, and tested their generalizability to the new task. The best fitting parameters ($\mu_{p}$ = 0,$\nu_{p}$ = 1, $\alpha_{p}$ = 1, $\beta_{p}$ = 1, $\epsilon$ = 0.0001) predicted the habituation and dishabituation in the training data ($R^2$ = 0.95 [0.90, 0.98]). Using these previously obtained parameters, we now test RANCH’s performance on the current experiment with different stimuli and design. We found that the parameters generalized to the current context well ($R^2$: `r paste(eig_gen_stats$rsquared, eig_gen_stats$r_conf_print)`, *RMSE*:  `r paste(eig_gen_stats$rmse, eig_gen_stats$rmse_conf_print)`). Moreover, RANCH also showed a qualitative ordering of the graded dishabituation similar to the behavioral data: animacy > identity > number > pose.  


## Parameter robustness 

To evaluate RANCH’s robustness to different parameters, we then conducted a grid search over the parameters, fitting the model to data from our  new experiment. We selected the best fitting parameters ($\mu_{p}$ = 0,$\nu_{p}$ = 2, $\alpha_{p}$ = 10, $\beta_{p}$ = 1, $\epsilon$ = 0.0001) using a 10-fold cross-validation on the behavioral dataset.  When fit to the full dataset, the best fitting parameter from this search was comparable with the parameters generalized from the training dataset ($R^2$: `r paste(eig_stats$rsquared, eig_stats$r_conf_print)`, *RMSE*:  `r paste(eig_stats$rmse, eig_stats$rmse_conf_print)`).  Moreover, performance across the `r n_params` parameter settings was relatively stable, yielding a moderate range of $R^2$ (*M* = `r eig_rsquared_mean`; *SD* = `r eig_rsquared_sd`) and RMSE (*M* = `r eig_rmse_mean`; *SD* = `r eig_rmse_sd`). Finally, the qualitative ordering of the dishabituation magnitude was also preserved when averaged across all parameter settings. 

## Comparison with alternative models

To examine whether the  three components — the perceptual representation, the learning model, and the decision model – in our models were critical to the success, we ran three alternative models: Random Embedding model, No Noise model, and No Learning Model. We ran a parameter search for each of the model, and all of the models showed worse performance compared to RANCH (Random Embedding: $R^2$: `r paste(randemb_stats$rsquared, randemb_stats$r_conf_print)`, *RMSE*:  `r paste(randemb_stats$rmse, randemb_stats$rmse_conf_print)`; No Noise: $R^2$: `r paste(nonoise_stats$rsquared, nonoise_stats$r_conf_print)`, *RMSE*: `r paste(nonoise_stats$rmse, nonoise_stats$rmse_conf_print)`; No Learning: $R^2$: `r paste(nolearning_stats$rsquared, nolearning_stats$r_conf_print)`, *RMSE*: `r paste(nolearning_stats$rmse, nolearning_stats$rmse_conf_print)`). 







