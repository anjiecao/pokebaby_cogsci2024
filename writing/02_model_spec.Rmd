

## Model Components

We modeled looking behaviors in our task as rational information acquisition, using a “rational analysis” approach previously described for different information-seeking behaviors [@oaksford1994rational; @dubey2020reconciling]. Our goal was to develop a model which formalizes the entire process underlying looking time: from perception of a stimulus to deciding how long to look at it. To do so, our model has three separate components describing (1) how stimuli are embedded a low-dimensional perceptual space (2) how RANCH learns a concept over this perceptual space and (3) how RANCH makes decisions about how long to sample from a stimulus based on its expected information gain. Here, we describe these three components in turn.


```{r design_fig, fig.env = "figure", fig.pos = "t", fig.align='center', fig.width=3.5, fig.height=3.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Workflow of the current study. (A) The experimental design and stimuli used in the training dataset. (B)The core components of RANCH, with the top showing the stimuli embedding in PC-space. The bottom is the plate diagram of the learning model. (C) The experimental design of the test data. RANCH model is first fit using the training data in A, and we test its prediction on the test data collected in the experiment illustrated in C."}
img <- png::readPNG("figs/train_test_fig.png")
grid::grid.raster(img)
```

### Perceptual representation 

To allow RANCH to operate on raw images, we used the perceptually-aligned embeddings obtained from a model presented recently by @lee2022rapid. We use these projections into a perceptually-aligned embedding space as a principled low-dimensional representation of stimuli, over which our learning model can form perceptual concepts. We used the first three principal components of the embedding space, which captured 57.9% of the variance. A visualization of experimental stimuli in the embedding space can be seen in Figure 1B.

### Learning model 

RANCH's goal is to learn a concept in the perceptual embedding space described above, through noisy perceptual samples from a stimulus. The concept is parameterized by a 3D Gaussian $(\mu,\sigma)$, which represents beliefs about the location and variance of the concept in the embedding space. This concept $(\mu,\sigma)$ generates exemplars $(y)$: exemplars of the concept. RANCH observes repeated noisy samples $(\bar{z})$ from each exemplar. For any sample $(z)$ from an exemplar $(y)$, the model expects the observation to get corrupted by zero-mean gaussian noise with standard deviation  $(\epsilon)$. A plate diagram is shown in Figure 1B. We used a normal-inverse-gamma prior on the concept, the conjugate prior for a normal with unknown mean and variance, on the concept parameterized as $\mu_{p}$,$\nu_{p}$,$\alpha_{p}$, $\beta_{p}$. Still, applying perceptual noise to $y$ breaks the conjugate relation, so we computed approximate posteriors using grid approximation over $(\mu,\sigma)$ and $(\epsilon)$. This computationally expensive approximation was accomplished through a PyTorch implementation and distributed GPU computation. 


### Decision model 

To decide whether to take an additional sample from the same stimulus, RANCH computes expected Information Gain (EIG) of the next sample. EIG is computed as the product of the posterior predictive probability of the next sample and the information gained conditioned on that next sample, by iterating through a grid of possible subsequent samples. RANCH then makes a softmax choice (with temperature = 1) between taking another sample and looking away. We assumed that participants expect a constant information gain from looking away (the “world EIG”). Therefore, as EIG from the stimulus drops below world EIG, it becomes increasingly likely that RANCH will look away.



## Alternative models 

To test the importance of each of RANCH’s components for its performance, we defined three lesioned models, in which one key feature of the model was removed. First, to test the importance of the perceptual embeddings, we ran a version of RANCH in which the mappings from stimulus labels to embeddings were permuted, such that the associations between embeddings and violation type were randomized (“Random embeddings”). Second, we ran a version in which RANCH assumes that each perceptual sample in the learning process is noiseless, rather than corrupted by $\epsilon$ (“No noise”).Third, we ran a version in which RANCH made decisions randomly rather than based on the learning model (“No learning”).

## Training Procedure 

We used the behavioral dataset reported in @cao2023habituation as the training data. In this prior experiment, 449 adults participated in an online self-paced looking time paradigm (Fig. 1A). In this paradigm, participants watched blocks of six animated monsters. Each block consisted of one repeating monster (the background), and one monster different from the repeated one (the deviant). The deviant, if shown, was on either the 2nd, 4th, or 6th trial of the block. Adults can proceed to the next trial whenever they press a key on the keyboard, and the interval between the onset of the stimuli and the key press was used as the proxy for looking time.  To train on this dataset, we first converted the raw stimuli into perceptual embeddings, and combined them into stimuli sequences identical to the sequence participants have seen. Since the model makes stochastic sampling decisions, we conducted 400 runs for each stimuli sequence for each parameter setting. To avoid numerical instabilities due to the granularity of our grid approximation, for each run we slightly jittered all parameter grid values by a constant offset. We conducted an iterative grid search across the priors over $\mu$, $\nu$ and $\epsilon$, along with the actual noise $\epsilon$. We selected the parameters that yielded the highest $R^2$ between the model output and the behavioral data as the best fitting parameters. The best fitting parameters were ($\mu_{p}$ = 0,$\nu_{p}$ = 1, $\alpha_{p}$ = 1, $\beta_{p}$ = 1, $\epsilon$ = 0.0001).


