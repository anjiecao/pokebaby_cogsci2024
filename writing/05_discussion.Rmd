In this paper, we report a novel experiment in which participants were familiarized to sequences of animations, and we measured habituation and their dishabituation to different types of deviations from familiar stimuli. We found that adults’ dishabituation is graded by the type of violation they see, and that the magnitude of dishabituation is predicted by a rational model which takes noisy samples from perceptual embeddings of the same stimuli. RANCH, through its use of perceptual embeddings, operates directly on raw images and therefore can generate predictions for previously unseen stimuli or even tasks. Making use of this property, RANCH successfully predicted human behaviors in our graded dishabituation task while using parameters fit to behaviors on a different task. 

```{r}
eig_stats <- readRDS(here("cached_data/writing_cache/clean_eig_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
nonoise_stats <- readRDS(here("cached_data/writing_cache/clean_nonoise_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
nolearning_stats <- readRDS(here("cached_data/writing_cache/clean_nolearning_stats.Rds"))%>% mutate(across(is.numeric, round, 2))
randemb_stats <- readRDS(here("cached_data/writing_cache/clean_randemb_stats.Rds"))%>% mutate(across(is.numeric, round, 2))

deltas <- c(eig_stats$rsquared - nonoise_stats$rsquared, 
            eig_stats$rsquared - nolearning_stats$rsquared, 
            eig_stats$rsquared - randemb_stats$rsquared)


```

Lesioning RANCH by removing key components caused its fit to the data to drop substantially relative to the full model ($\Delta_{R^2}$ of `r min(deltas)` - `r max(deltas)`) and eliminated any sense of qualitative correspondence to the human data. This result suggests that the aspects that we lesioned - a psychologically-plausible embedding space, noisy perception, connecting sampling to concept learning - are all essential for explaining behaviors in our task.

There are several directions in which RANCH could be extended in future work. First, in the current paper, we implemented a specific version of RANCH, with a specific form for each of its components: the perceptual representation, the learning model, and the linking hypothesis between learning and attentional sampling. However, RANCH’s modular and interpretable structure allows researchers to adjust its components according to the population or task for which predictions are being generated. For example, the perceptual embedding space used in this paper was aligned to adult behavior [@lee2022rapid], but infants likely represent visual objects differently from adults. Using perceptual representations based on visual input experienced by infants may provide a better fit to infant data [@zhuang2021unsupervised; @orhan2023can]. Similarly, task settings in which there was hierarchical structure to the stimulus sequences would call for more complicated learning models. The linking hypothesis can also vary. For example, the rational, but computationally expensive, EIG could be replaced with easier-to-compute information-theoretic quantities such as surprisal or KL-divergence [@cao2023habituation; @raz2023modeling]. While previous work has found that these linking hypotheses make similar predictions, it is possible that they may dissociate under different task settings or different assumptions about the perceptual representations. In summary, RANCH’s modularity offers a rich space of hypotheses about possible computations underlying looking behavior. 

Second, while inspired by infant looking time research, our current work only has adult participants. Beyond encoding stimuli differently from infants, adults may conceptualize our task differently from infants, and experience different task demands. In particular, infants are quite sensitive to changes in the number of objects that are displayed [@wynn1992addition; @feigenson2003tracking], but in the current study, adults dishabituated to number violations as little as to pose violations, the subtlest violation in our task. This suggests that adults may not have engaged number cognition in this task, as infants likely would. Furthermore, given the interpretability of the model parameters we fit to the behaviors, conducting the same experiment with infants may lead to interpretable developmental differences in the model parameters, such as priors on perceptual noise and prior uncertainty about the mean and standard deviation of perceptual concepts.

Overall, our work presents a rational model, RANCH, which describes how humans decide how long to look at stimuli. Using a psychologically motivated visual encoding model allows RANCH to operate on raw images, and generate predictions for previously unseen stimuli or tasks. We think that the generality and interpretability of our model framework constitutes a significant step towards predictive modeling of adult, and eventually infant, looking time, thereby putting the field on firmer ground.
